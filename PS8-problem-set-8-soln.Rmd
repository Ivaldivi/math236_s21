=
# Problem Set 8

* Due: Tuesday December 15 by 11:55pm CST. 
* Upload your solutions to Moodle in a PDF. 
* Please feel free to **use RStudio for all calculations, including row reduction, matrix multiplication, eigenvector calculation, inverse matrices, and the Gram-Schmidt process.**
* You can download the [Rmd source file  for this problem set](https://github.com/mathbeveridge/math236_f20/blob/main/PS8-problem-set-8.Rmd).

This problem set covers Sections 6.1 - 6.5 Orthogonality.


## Two Orthogonality Properties

Give rigorous proofs for each of the following problems. Here "rigorous" means that the heart of your explanation must use equations and calculations to explicitly justify any intuitive reasoning that you provide.

a. Let $W$ be a subspace of $\mathbb{R}^5$ with basis $\mathsf{w}_1, \mathsf{w}_2, \mathsf{w}_3$.  Let $W^{\perp}$ be its orthogonal complement with basis $\mathsf{v}_1, \mathsf{v}_2$.  Prove that if $\mathsf{u}$ is in both $W$ and $W^{\perp}$ then $\mathsf{u}$ must be the zero vector. 

a. Let $U$ and $V$ both be $n \times n$ orthogonal matrices. Prove that the matrix $UV$ is also an orthogonal matrix.

POSSIBLE EXAM QUESTION
Suppose that $A = PCP^{-1}$ where $C$ is a scaling-rotation matrix and $P$ is an orthonormal matrix. Then $A$ is also a scaling-rotation matrx.

A $2 \times 2$ matrix $P$ with orthogonal columns is a scaling-rotation matrix.




```{r}
B = cbind(c(3,1),c(-1,3))
P = cbind(c(1,-4),c(4,1))
Pinv = solve(P)

P %*% B %*% Pinv

```



## Bases for a Subspace and its Orthogonal Complement

Let $W \subset \mathbb{R}^6$ be the set of solutions to
\begin{align}
x_1 + x_2 + x_3 + x_4 + x_5 + x_6 &= 0 \\
x_1 - x_2 + x_3 - x_4 + x_5 - x_6 &= 0 
\end{align}

a. Find a basis for $W$.
b. Use your answer from (a) to find an orthogonal basis for $W$ where the entries have integer values.
c. Find a basis for $W^{\perp}$
d. Use your answer from (c) to find an orthogonal basis for $W^{\perp}$ where the entries have integer values.






```{r}
library(pracma)
A = rbind(c(1,1,1,1,1,1),
          c(1,-1,1,-1,1,-1))

rref(A)

v1 = c(-1,0,1,0,0,0)
v2 = c(0,-1,0,1,0,0)
v3 = c(-1,0,0,0,1,0)
v4 = c(0,-1,0,0,0,1)

B = cbind(v1,v2,v3,v4)
gramSchmidt(B)

t(A)

gramSchmidt(t(A))
```
```{r}
library(pracma)
A = rbind(c(1,1,1,1,1,1),
          c(1,1,-1,1,1,-1))

rref(A)

v1 = c(-1,0,1,0,0,0)
v2 = c(-1,0,0,1,0,0)
v3 = c(-1,0,0,0,1,0)
v4 = c(0,-1,0,0,0,1)

B = cbind(v1,v2,v3,v4)
gramSchmidt(B)

t(A)

gramSchmidt(t(A))
```


## Eigensystem of a Symmetrix Matrix

Recall that a square $n \times n$ matrix is symmetric when $A^{\top} = A$. We learned that the eigenvectors of a symmetric matric form an orthogonal basis of $\mathbb{R}^n$. In this problem, you will confirm that this holds for the following symmetric matrix

$$
A = 
\begin{bmatrix}
 0 &  8 & 10 & -4 \\
 8 & 4 & 28 & 6 \\
 10 & 28 & 3 & -4 \\
 -4 & 6 & -4 & -7
\end{bmatrix}
. 
$$

1. Find the eigenvalues and eigenvectors of $A$.

2. Confirm that the eigenvectors returned by R are an orthonormal set. (Pro tip: you can do this in one calculation!)

3. Express the vector $\mathsf{v} = \begin{bmatrix} 4 & -6 &  8  & 1 \end{bmatrix}^{\top}$ as a linear combination of the eigenvectors. Use the fact that the eigenvectors are orthonormal. (Don't augment and row reduce.)

4.  Let $P$ be the matrix of these normalized, orthogonal eigenvectors.
Diagonalize $A$ using $P$. Just write out $A = P D P^{-1}$. Congratulations: you have **orthogonally diagonalized** the symmetric matrix $A$.



```{r}

#A = cbind(c(5,2,9,-6), c(2,5,-6,9), c(9,-6,5,2), c(-6,9,2,5))

#eigen(A)

#B = cbind(c(10,2,2,-6,9), c(2,10,2,-6,9), c(2,2,10,-6,9), c(-6,-6,-6,26, 9), c(9,9,9,9,-19))

#eigen(B)


v1 = 1/3 * c(1,0,2,-2)
v2 = 1/3 *c(2,2,-1,0)
v3 = 1/3 *c(2,-2,0,1)
v4 = 1/3 *c(0,-1,-2,-2)
A = cbind(v1, v2, v3, v4)

D = diag(c(4,-3,0,-1)) *9

B = t(A) %*% D %*% A 

B

 eigen(B)


 
 
#vecs = syst$vectors

#syst


#zapsmall(t(vecs) %*% vecs)



```

```{r}

 6 * v1 - 4 * v2 + 7 * v3 - 4* v4
```





## Automotive Fuel Efficiency in 1974

One of the built-in datasets in RStudio is comes from a 1974 Motor Trend magazine article about the design and performance for 32 cars. Use least squares approximation to find the best fitting linear function 
<center>
mpg = $c_1$ + $c_2$  cyl + $c_3$  hp + $c_4$  wt + $c_5$  gear  
</center>
where 

* mpg is gas milage (miles per gallon)
* cyl is the the number of cylinders
* hp is the the horsepower of the engine
* wt is the weight of the car (in 1000 pounds), and 
* gear is the number of gears. 

1. Find the constants $\begin{bmatrix} c_1 & c_2 & c_3 & c_4 & c_5 \end{bmatrix}$.

2. Find the residual vector  and verify that it is orthogonal to the data vectors cyl, hp, wt and gear.

3. Which of these features has a negative coefficient? Which have a positive coefficient? What does that mean with respect to fuel efficiency?

 

```{r}
mpg = mtcars[,1]
cyl = mtcars[,2]
hp = mtcars[,4]
wt = mtcars[,6]
gear = mtcars[,10]

#cyl = cyl/max(cyl)
#hp = hp/max(hp)
#wt = wt/max(hp)
#

gear = gear/max(gear)

A = cbind(rep(1,32), cyl, hp, wt, gear)
A

(xhat = solve(t(A) %*% A, t(A) %*% mpg))

```



## Modeling Fertility in 1888 Switzerland

RStudio comes with some sample datasets, including a classic Swiss dataset collected in 1888. Switzerland was beginning demographic transition: birth rates were beginning to decrease as the country entered the industrial age. 

This data set contains the following columns, all scaled to the range $[0,100]$. The rows correspond to 47 French-speaking pronvinces in 1888.

* Fertility `fert`: standardized fertility rate
* Agriculture `agr`: percent of men in agricultural occupations
* Examination `exam`: percent of draftees receiving highest mark on army examination
* Education `educ`: percent education beyond primary school for draftees
* Catholic `cath`: percent Catholic (instead of Protestant)
* Infant.Mortality `mort`: percent of live births who live less than one year

We want to find the best-fit linear model
<center>
fert = $c_1$ + $c_2$ agr + $c_3$ exam + $c_4$ educ + $c_5$ cath + $c_6$ mort.
</center>

Obviously, this data set shows its age. We are attempting to model fertility rates of **women** by using indirect measures about **male occupation and education levels**! This is problematic and that must be addressed in a thorough analysis. Here, we use this classic data to illustrate the method of least squares.

a. Model this problem as a least-squares approximation of an inconsistent system $A \mathsf{x} = \mathsf{y}$. Find the least squares solution $\hat{\mathsf{x}}$ and confirm that the residual vector $\mathsf{z}$ is orthogonal to the columns of $A$.


```{r}
A = data.matrix(swiss[,2:6])
A = cbind(rep(1, 47), A)
y = data.matrix(swiss[,1])

xhat = solve(t(A) %*% A, t(A) %*% y)
xhat
yhat = A %*% xhat
z = y - yhat

x = t(A) %*% z

```

```{r}
t(A) %*% z
```

b. We know that the residal vector $\mathsf{z}$ is a measure of the quality of fit of our model. But how do we turn this into a meaninful quantity?
One method is to look at the **coefficient of determination**, which is more commonly refered to as the "$R^2$ value."

Let $\mathsf{y} = [] y_1, y_2, \ldots, y_n ]^{\top}$ be our target vector with least squares solution $\hat{\mathsf{y}} = A \hat{\mathsf{x}$ and residual vector $\mathsf{z} = \mathsf{y} - \hat{\mathsf{y}}$. Let 
$$
a = \frac{1}{n} ( y_1 + y_2 + \cdots + y_n)
$$
be the average of the entries of target vector $\mathsf{y}$
and let $\overline{\mathsf{y}} = [a, a, \ldots, a]$. (We call this vector "y bar", so `ybar` would be a fine name in R.)
The $R^2$ value is
$$
R^2 = 1 - \frac{\| \mathsf{y} - \hat{\mathsf{y}} \| }{\| \mathsf{y} - \bar{\mathsf{y}} \|}
$$
This is a number in $[0,1]$. If the $R^2$ value is near 1, then our model does a good job at "explaining" the behavior of $\mathsf{y}$ via a linear combination of the columns of $A$. 

Find the $R^2$ value for our least squares solution to the Swiss data. Here are two helpful functions:

*  `sum(vec)` returns the sum of the entries of the vector `vec`
* `length(vec)` returns the number of entries in the vector `vec`
* `rep(a, n)` creates a constant vector of length $n$ where every entry is $a$.

```{r}

a = sum(y)/length(y)
ybar = rep(a, length(y))

rsquared = 1 - get_length(y - yhat)/get_length(y - ybar)
rsquared

```


**Remark:** There is certainly more to say about $R^2$ and best practices around fitting models to data. To learn more, you should take STAT 155 Introduction to Statistical Modeling. 




## Cosine Similarity for US Senators

In high dimensional space $\mathbb{R}^n$ a common measure of similarity between two vectors  is  \emph{cosine similarity}:  the cosine of the angle $\theta$ between the vectors. We calculate this value as follows:
$$
\cos(\theta) = \frac{ \mathsf{v} \cdot \mathsf{w}} {\| \mathsf{v}\| \, \|\mathsf{w}\|} = \frac{ \mathsf{v} \cdot \mathsf{w}} {\sqrt{\mathsf{v} \cdot \mathsf{v}} \sqrt{\mathsf{w} \cdot \mathsf{w}}}.
$$
This measure has the following nice properties:

* $-1 \le \cos(\theta) \le 1$,
* $\cos(\theta)$ is close to 1 if $\mathsf{u}$ and $\mathsf{v}$ are closely aligned,
* $\cos(\theta)$ is close to 0 if  $\mathsf{u}$ and $\mathsf{v}$ are are orthogonal, 
* $\cos(\theta)$ is close to $-1$ if $\mathsf{u}$ and $\mathsf{v}$ are polar opposites.

Let's use cosine similarity to compare the 99 US Senators (one senate seat was not filled at the time) from the 109th US Congress (2007-2008).

Here is code that loads in the voting records. Each row contains the senator's name, party affiliation, state, and then their record on 46 resolutions. The votes are encoded as a sequence of
0s, 1s, and -1s, where 1 means a `aye' vote, -1 means a `nay' vote, and 0 means the senator abstained. The row corresponding to Joseph Biden is
<center>
`biden = ["Biden", "D", "DE", -1, -1, 1, 1, ... , 1, -1]`.
</center>




```{r, echo=TRUE}

library(readr)

senate.vote.file = "data/SenateVoting109.csv"

record <- read_csv(senate.vote.file, col_names = TRUE)

clinton = record[record$Name == 'Clinton',]
collins = record[record$Name == 'Collins',]
frist = record[record$Name == 'Frist',]
mccain = record[record$Name == 'McCain',]
obama = record[record$Name == 'Obama',]
reid = record[record$Name == 'Reid',]

record

```

Next, here is code that you can use 

* `get_votes(senator)` is a helper function that returns the vote vector for the senator. These are the values in columns 3 to 49.
* `get_vote_length(senator)` returns the length of the senator's vote vector
* `get_vote_cosine_similarity(senator1, senator2)` returns the cosine of the angle between the vote vectors of `senator1` and `senator2`



```{r, echo=TRUE}


# returns the vote vector for the senator
get_votes <- function(senator) {
  #unlist makes this dataframe row into a regular vector
  return (unlist(senator[,4:49])) 
}

# returns the length of the senator's vote vector
get_vote_length <- function(senator) {
  votes = get_votes(senator)
  return (sqrt(t(votes) %*% votes))
}

# returns the cosine of the angle between the vote vectors 
# of senator1 and senator2
get_vote_cosine_similarity <- function(senator1, senator2) {
  votes1 = get_votes(senator1)
  votes2 = get_votes(senator2)
  numer = t(votes1) %*% votes2
  denom = get_vote_length(senator1) * get_vote_length(senator2)
  
  return (numer/denom)
}

```


1. Find the cosine of the angles between every pair of the following senators of note:

    + `clinton`: Hilary Clinton (D, NY), presidential candidate 2016
    + `mccain`: John McCain (R, AZ), presidential candidate 2008
    + `obama`: Barack Obama (D, IL), president 2008-2016
    + `collins`: Susan Collins (R, ME), moderate Republican

  Does the cosine similarity pick up on the fact that Senator Collins is a "moderate Republican"?
  
```{r}
get_vote_cosine_similarity(clinton, collins)
get_vote_cosine_similarity(clinton, mccain)
get_vote_cosine_similarity(clinton, obama)

get_vote_cosine_similarity(collins, mccain)
get_vote_cosine_similarity(collins, obama)

get_vote_cosine_similarity(mccain, obama)

```


2. The senate majority leader of the 109th Congress was Bill Frist (R, TN). The senate minority leader was Harry Reid (D, NV). Create a function  `classify_senator(senator)` that returns "R" or "D" depending on the cosine similarity of `senator` to `frist` and to `reid`. The run the code below to see how many senators are "misclassified," meaning that their votes are more similar to the leader of the opposing party. 

**Note:** Your list will include Jim Jeffords (I, VT), a Republican who became and Independent in 2001 and then caucused with the Democrats. Your classifer correctly aligns the Independent Jeffords with the Democrats.




```{r}


classify_senator <- function(senator) {
  cosR = get_vote_cosine_similarity(senator, frist)
  cosD = get_vote_cosine_similarity(senator, reid)
  if (cosR > cosD) {
    return ("R")
  } else {
    return ("D")
  }
}

mismatch = vector()

for (i in 1:99) {
  senator = record[i,]
  party = classify_senator(senator)
  if (party !=  record[i,2]) {
    mismatch = c(mismatch, i)
  }
}

record[mismatch,1:2]


```
