

# Quiz 4 Review


## Overview

Our fourth quiz covers sections 6.1-6.5, 7.1 and 7.4  in Lay's book. This corresponds to Problem Set 8.

The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice:

* Make sure that you have mastered the Vocabulary, Skills and Concepts  listed below.
* Look over the Edfinity homework assingments
* Do practice problems from the Edfinity Practice assignments. These allow you to "Practice Similar" by generating new variations of the same problem.
* Redo the Jamboard problems
* Try to resolve the Problem Sets and compare your answers to the solutions.
* Do the practice problems below. Compare your answers to the solutions.

### Vocabulary, Concepts and Skills

See the [Week 7-8 Learning Goals](week-7-8-learning-goals) for the list of vocabulary, concepts and skills.



## Practice Problems


###

Let $\mathsf{v} =  \begin{bmatrix}1 \\ -1 \\ 1 \end{bmatrix}$ and $\mathsf{w}= \begin{bmatrix}5 \\ 2 \\ 3 \end{bmatrix}$.


a. Find $\| \mathsf{v} \|$ and $\| \mathsf{w} \|$.

a. Find the distance between $\mathsf{v}$ and $\mathsf{w}$.

a. Find the cosine of the angle between $\mathsf{v}$ and $\mathsf{v}$.

a. Find $\mbox{proj}_{\mathsf{v}} \mathsf{w}$. 


b. Let $W=\mbox{span} (\mathsf{v}, \mathsf{w})$. Create an orthonormal basis $\mathsf{u}_1, \mathsf{u}_2$ for $W$ such that $\mathsf{u}_1$ is a vector in the same direction as $\mathsf{v}$.


###

Let $\mathsf{u} \neq 0$ be a vector in $\mathbb{R}^n$. Define the function $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ by
$T(\mathsf{x}) = \mbox{proj}_{\mathsf{u}} \mathsf{x}$. 

a. Prove that $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ is a linear transformation.

b. Recall that the kernel of $T$ is the subspace $\mbox{ker}(T) = \{ \mathsf{x} \in \mathbb{R}^n \mid T(x) = \mathbf{0} \}$. Describe  $\mbox{ker}(T)$ as explicitly as you can.




### 

 The vectors $\mathsf{u}_1, \mathsf{u}_2$  form an orthonormal basis of a subspace $W$ of $\mathbb{R}^4$. Find the projection of $\mathsf{v}$ onto $W$ and determine how close $\mathsf{v}$ is to $W$.
$$
\mathsf{u}_1 = \frac{1}{2}\begin{bmatrix} 1\\ -1\\ -1\\ 1 \end{bmatrix}, \quad
\mathsf{u}_2 =  \frac{1}{2}\begin{bmatrix} 1\\ -1\\ 1\\ -1  \end{bmatrix}, \quad
\mathsf{v} =  \begin{bmatrix}   2\\ 2\\ 4\\ 2 \end{bmatrix} 
$$

###



Consider vectors 
$\mathsf{v}_1 = \begin{bmatrix} 1 \\ 1 \\-1 \end{bmatrix}$ and 
$\mathsf{v}_2= \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ in $\mathbb{R}^3$.
Let $W=\mbox{span}(\mathsf{v}_1, \mathsf{v}_2)$.

a.  Show that $\mathsf{v}_1$ and $\mathsf{v}_2$ are orthogonal.

b.  Find a basis for $W^{\perp}$.

c. Use orthogonal projections to find the representation of 
$\mathsf{y} = \begin{bmatrix} 8 \\ 0 \\ 2 \end{bmatrix}$ as
$\mathsf{y} = \hat{\mathsf{y}} + \mathsf{z}$ where
$\hat{\mathsf{y}} \in W$ and $\mathsf{z} \in W^{\perp}$.



###
Let $W$ be the span of the vectors
$$
\begin{bmatrix}
1 \\ -2 \\ 1 \\ 0 \\1 
\end{bmatrix}, \quad
\begin{bmatrix}
-1 \\ 3 \\ -1 \\ 1 \\ -1 
\end{bmatrix}, \quad
\begin{bmatrix}
0 \\ 0 \\ 1 \\ 3 \\1 
\end{bmatrix}, \quad
\begin{bmatrix}
0 \\ 2 \\ 0 \\ 0 \\4 
\end{bmatrix}
$$

a. Find a basis for $W$. What is the dimension of this subspace?
b. Use the Gram-Schmidt process on your answer to part (a) to find an orthonormal basis for $W$
c. Find a basis for $W^{\perp}$
d. Use the Gram-Schmidt process on your answer from part (c) to find an orthogonal basis for $W^{\perp}$.


### 

Let $\mathsf{u}_1, \mathsf{u}_1, \ldots, \mathsf{u}_n$ be an **orthonormal** basis for $\mathbb{R}^n$. Pick any $\mathsf{v} \in \mathbb{R}^n$. Show that
$$
\| \mathsf{v} \| = \sqrt{ ( \mathsf{v} \cdot \mathsf{u}_1)^2 + (\mathsf{v} \cdot \mathsf{u}_1)^2 + \cdots +(\mathsf{v} \cdot \mathsf{u}_1)^2}.
$$

<!--
* Use the Gram-Schmidt process to find an orthogonal basis for $W$.
* Find a basis for $W^{\perp}

```
A = cbind(c(3,0,0,4,0),c(1,1,0,-1,1),c(0,1,2,-1,2))
A = cbind(c(1,-2,1,0,1),c(-1,3,-1,1,-1),c(1,0,1,4,5),c(0,2,0,0,-4))

gramSchmidt(A)
```

-->


<!-- least squares -->

###

Consider the system $A \mathsf{x} = \mathsf{b}$ given by
$$
\begin{bmatrix}
1 & 1 & 1 \\
1 & 2 & -1 \\
1 & 1 & -1 \\
1 & 2 & 1
\end{bmatrix}
\begin{bmatrix}
x_1\\ x_2 \\ x_3
\end{bmatrix}
=
\begin{bmatrix}
4\\ 1 \\ -2 \\ -1
\end{bmatrix}.
$$

a. Show that this system is inconsistent.
b. Find the projected value $\hat{\mathsf{b}}$,  and the residual $\mathsf{z}$.


###

Here is an inconsistent system of equations:
$$
\begin{bmatrix} 1 & 2 \\ 1 & 2 \\ 1 & -1 \end{bmatrix}
 \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 
  \begin{bmatrix} 6\\ 4 \\ -4 \end{bmatrix} 
$$

a. State the normal equations for this problem (be sure to do all of the necessary matrix multiplications).

b. Find the least squares solution to the problem.

c. How close is your approximate solution to the desired target vector?

### 



Curve Fitting problem coming soon!

###

Consider the symmetric matrix 
$$
A = \begin{bmatrix}

   3  &  0 & 34  &  3 \\
   0  &  6 & -34  &  0 \\
  34 & -34 &  74  & 34 \\
   3  &  0  & 34  &  3
\end{bmatrix}
$$
a. Use RStudio to find the eigenvalues $\lambda_1 > \lambda_2 > \lambda_3 > \lambda_4$ and their corresponding eigenvectors $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3, \mathsf{v}_3. Confirm that these eigenvectors form an orthonormal set.

b. Is the linear transformation $T(\mathsf{x}) = Ax$ invertible? How do you know?

c. Confirm that $A$ = $\lambda_{1} \mathsf{v}_1 \mathsf{v}_1^{\top} + \lambda_{2} \mathsf{v}_2 \mathsf{v}_2^{\top} + \lambda_{3} \mathsf{v}_3 \mathsf{v}_3^{\top} + \lambda_{4} \mathsf{v}_4 \mathsf{v}_4^{\top}.

d. Use your answer in part (d) to find the best rank 2 approximation for $A$. (Be careful!)

<!--
```{r}
#A = cbind(c(20,1,1), c(1,17,2), c(1,2,17))
#A
#eigen(A)
#%svd(A)


P = cbind(c(1,-1,-1,1),c(1,2,0,1), c(1,0,0,-1),c(1,-1,3,1))

D = diag(c(-7,1,0,9))


A = P %*% D %*% t(P)

A

syst = eigen(A)

v1 = syst$vectors[,1]
v2 = syst$vectors[,2]
v3 = syst$vectors[,3]
v4 = syst$vectors[,4]

syst$values[1] * v1 %*% t(v1) + syst$values[2] * v2 %*% t(v2) + syst$values[3] * v3 %*% t(v3) + syst$values[4] * v4 %*% t(v4)

syst$values[1] * v1 %*% t(v1)  + syst$values[4] * v4 %*% t(v4)

zapsmall(syst$values[2] * v2 %*% t(v2) + syst$values[3] * v3 %*% t(v3))

```
-->






###

Here is a matrix $A$ and its reduced row echelon form $B$
$$
A = \begin{bmatrix}
   1  &  3  & -3  &  1  &  0 \\
    2  &  1  &  0  &  6 &   5 \\
   3  &  3  & -3  &  6  &  3 \\
  -1  &  4  & -3  & -3  & -1
\end{bmatrix}
\qquad \longrightarrow \qquad
B = \begin{bmatrix}
    1 &   0  &  0 & 2.5 & 1.5 \\
    0  &  1  &  0 & 1.0 & 2.0 \\
  0  & 0  &  1 & 1.5 & 2.5 \\
   0  &  0  &  0 & 0.0 & 0.0
\end{bmatrix}
$$

a. Find a basis for $\mbox{Nul}(A)$ and $\mbox{Col}(A)$.

b. Is the linear transformation $T(\mathsf{x}) = A \mathsf{x}$ one-to-one? Onto?

c. How is the SVD for $A$ related to the SVD for $B$? What properties will they share? What properties will be different? Make some conjectures.

d. Now find the SVD of both $A$ and $B$, and test your conjectures. Compare the singular values, the right singular vectors and the left singular vectors. Be sure to compare each of the four fundamental subspaces: $\mbox{Nul}(M), \mbox{Col}(M), \mbox{Row}(M), \mbox{Nul}(M^{\top})$. 

<!--
```
v1 = c(1,3,-3,0,0)
v2 = c(1,-2,4,2,-1)
v3 = c(0,1,0,3,2)
v4 = c(2,0,0,-1,3)

w1 = v1+v2
w2 = v4-v2+v3
w3 = v4+v1
w4 = v3 + v1-v4


v1 = c(1,3,-3,1,0)
#v2 = c(1,-2,4,0,-1)
v3 = c(0,1,0,1,2)
v4 = c(2,0,0,5,3)
v2 = c(0,0,0,0,0)

w1 = v1+v2
w2 = v4-v2+v3
w3 = v4+v1
w4 = v3 + v1-v4

B = rbind(w1,w2,w3,w4)
B
#U = rref(B)

U

svd(B)
svd(U)
```
-->

### 

Another SVD problem coming soon


###

Applied SVD problem coming soon

